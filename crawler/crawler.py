from collections import OrderedDict
from typing import List


class Crawler:
    """ Класс анализирует текстовый контент веб-страниц, начиная с заданного списка URL-ов, и бьёт его на абзацы.

    """
    def load_and_tokenize(self, urls: List[str], depth: int=3) -> OrderedDict:
        """ Загрузить все веб-страницы по заданным URL-ам и распарсить текстовый контент в них.

        Производится парсинг как веб-страниц, заданных исходным списком URL-ов, так рекурсивный обход всех других
        веб-страниц, на которые можно перейти из исходных. Текстовый контент каждой страницы структурируется путём
        разбивки на отдельные абзацы. В результате работы функции возвращается словарь, ключами которого являются
        строковые описания URL-ов, обойдённых в процессе парсинга, а значениями - списки строк, т.е. текстовый контент
        каждого URL-а, разбитый на последовательность абзацев. Пример:

        {
            "www.abc.com": [
                "мама мыла раму",
                "корова молоко даёт"
            ],
            "https://hello.org": [
                "Здравствуй, мир!",
                "И тебе исполать, добрый молодец!",
                "Доброго здоровьица, девица краса.",
                "Здесь что, все здороваются?"
            ]
        }

        :param urls: список стартовых URL-ов
        :param depth: глубина рекурсивного обхода, начиная со стартовых URL-ов

        :return словарь текстового контента, разбитого на абзацы, для всех обойдённых URL-ов

        """
        pass